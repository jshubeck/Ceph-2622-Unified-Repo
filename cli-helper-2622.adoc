# TXC 2025 - Lab 2622 Helper Commands
//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4

## Introduction

### About IBM Storage Ceph

### About this Lab

### High Level Architecture

### Accessing your Lab environment

Accessing your Lab Workstation (the physical laptop) via ssh:

[source, shell]
----
export USERNAME={your_User_Name}
export USERIP={your_Bastion_Public_IP_Address}
chmod 0600 ~/Downloads/ssh_private_key.pem
ssh -i ~/Downloads/ssh_private_key.pem -p 2223 ${USERNAME}@${USERIP}
----

Accessing your Workstation (the physical laptop) using a browser via CLI

**Windows:**

[source, shell]
----
cmd.exe
set USERIP={your_Bastion_Public_IP_Address}
start https://${your_Bastion_Public_IP_Address}
----

**MacOS/Linux:**

[source, shell]
----
set USERIP={your_Bastion_Public_IP_Address}
open -a "Google Chrome" "https://${USERIP}"
open -a "Safari" "https://${USERIP}"
----

## IBM Storage Ceph configuration using the Dashboard

### Introduction

### Overview of the Ceph Dashboard

### Connect to the Jump Workstation Desktop

### Set the Ceph Dashboard admin password

### Expand the cluster

### Add nodes

.Generate a multiple host string for node expansion
[source, shell]
----
grep node1 /etc/hosts | awk '{ print $2 }' | sed -e 's/node1-/node[2-4]-/g'
----

### Add storage devices and OSDs

### Accept Grafana self-signed cert to view performance details

.Generate URL for Certificate exception
[source, shell]
----
grep node1 /etc/hosts | awk '{ print $2 }'| sed -e 's#ceph-node1-#https://ceph-node1-#g' | sed -e 's/$/:3000/'
----


## Configure IBM Storage Ceph Nodes, Pools and services

### Label hosts for specific services

### Configure IBM Storage Ceph services

#### Deploy the IBM Storage Ceph Object (RGW) service

#### Deploy the Network File System (NFS) service

### Configure Ceph Pools

#### RADOS Gateway Pool Creation

## Configure Ceph block storage

### Configure Ceph RADOS Block Device

#### Create an RBD Image

### RBD client experience

#### Access the RBD image using the Linux kernel driver

.Sudo to the root user, and then verify RBD access
[source, shell]
----
sudo -i
rbd ls
----

.Verify the parameters or the RBD image
[source, shell]
----
rbd info rbdimage
----

.Map the new virtual block device to this client node
[source, shell]
----
rbd map rbdimage
rbd showmapped
----

#### Map the RBD image to file system access

.Format the device with an XFS file system
[source, shell]
----
mkfs.xfs /dev/rbd0
----

.Create a mountpoint
[source, shell]
----
mkdir /mnt/rbdimage
----

.Mount the newly formatted rbdimage file system
[source, shell]
----
mount /dev/rbd0 /mnt/rbdimage
----

#### Client read and write operations

.Create a directory and add files to the rbdimage
[source, shell]
----
mkdir /mnt/rbdimage/dir1
mkdir /mnt/rbdimage/dir2
touch /mnt/rbdimage/dir1/atestfile
dd if=/dev/random of=/mnt/rbdimage/dir1/10MB.dat bs=10M count=1
echo "Hello world" > /mnt/rbdimage/dir2/hello-world.txt
cat /mnt/rbdimage/dir2/hello-world.txt
----

.Review the file system usage
[source, shell]
----
ls -al /mnt/rbdimage/dir1
----

.Review the mount point directory and file tree
[source, shell]
----
tree /mnt/rbdimage
----

#### Cephadm verify and cleanup

.Ceph file system command
[source, shell]
----
ceph df
----

.Cleanup commands
[source, shell]
----
umount /mnt/rbdimage
exit
whoami
----

## Configure Ceph file storage

### Configure Ceph file system

#### Create a CephFS volume

#### Create a Subvolume group

#### Create a Subvolume

#### Create an NFS export

### Ceph file system client experience

#### Mount CephFS using the Linux kernel driver

.Sudo to the root user, and create a mount point
[source, shell]
----
sudo -i
mkdir /mnt/fsdemo
tree /mnt/fsdemo
----

.Mount the Ceph file system
[source, shell]
----
export SVOL=<paste-from-clipboard>
echo $SVOL
mount -t cephfs ceph-node3:$SVOL /mnt/fsdemo -o name=admin
df | grep /mnt/fsdemo
----

#### Client read and write operations

.Read and write data to the mounted CephFS Subvolume
[source, shell]
----
mkdir /mnt/fsdemo/dir1
mkdir /mnt/fsdemo/dir2
touch /mnt/fsdemo/dir1/atestfile
dd if=/dev/random of=/mnt/fsdemo/dir1/10MB.dat bs=1MB count=10
echo "echo “98333, Fox Island, WA” > /mnt/fsdemo/dir2/zip-codes.csv
----

.Review the file system usage
[source, shell]
----
ls -al /mnt/fsdemo/dir1
----

.Review the mount point directory and file tree
[source, shell]
----
tree /mnt/fsdemo
----

#### Cephadm verify and cleanup

.Review the CephFS volume usage in the Ceph cluster
[source, shell]
----
ceph df
----

.Unmount the file system
[source, shell]
----
umount fsdemo
----

#### Mount CephFS using the NFS client

.Create a mount point for NFS
[source, shell]
----
whoami
mkdir /mnt/nfsdemo
----

.Mount the NFS export of the Cephfs subvolume
[source, shell]
----
/sbin/mount.nfs ceph-node3:/nfsvol1 /mnt/nfsdemo
df | grep /mnt/nfsdemo
----

.Read and write new data to the NFS mount
[source, shell]
----
cp ~/Cotton.jpeg /mnt/nfsdemo/dir2
ls -al /mnt/nfsdemo/dir2
----

.Review the mount point directory and file tree
[source, shell]
----
tree /mnt/nfsdemo
----

.Unmount the file system and return to the initial user
[source, shell]
----
umount /mnt/fsdemo
exit
whoami
----


## Working with object storage

### Configure object storage users and buckets

#### Create an S3 user

#### Provision an S3 bucket

### Client interaction and data access

#### Using the AWS CLI S3 client

.Retrieve and display the access keys from the Ceph RGW service
[source, shell]
----
ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0] | .access_key, .secret_key '
----

.Set the access key and secret access key
[source, shell]
----
export AKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].access_key');echo $AKEY
export SKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].secret_key');echo $SKEY
aws configure set aws_access_key_id $AKEY --profile demouser
aws configure set aws_secret_access_key $SKEY --profile demouser
----

.Set the address of the S3 endpoint
[source, shell]
----
aws configure set endpoint_url http://ceph-node4 --profile demouser
aws configure set region multizg --profile demouser
----

.Configure a command line alias for convenience
[source, shell]
----
alias aws="aws --profile demouser"
----

.List the available S3 buckets
[source, shell]
----
aws s3 ls
----

.Create a 10 MB file and upload it to the bucket
[source, shell]
----
dd if=/dev/random of=10MB.bin bs=1M count=10
aws --acl=public-read-write s3 cp ./10MB.bin s3://demo-bucket/10MB.bin
----

.Get a bucket listing to view the test object and download it
[source, shell]
----
aws s3 ls s3://demo-bucket
aws s3 cp s3://demo-bucket/10MB.bin GET-10MB.bin 
----

.Verify the data integrity of the uploaded and downloaded files by comparing their checksums
[source, shell]
----
md5sum 10MB.bin
md5sum GET-10MB.bin
----

#### Using the Minio Client

.Configure the MinIO Client with the access key, secret key, and endpoint values
[source, shell]
----
export AKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].access_key');echo $AKEY
export SKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].secret_key');echo $SKEY
mc alias set demouser http://ceph-node4 $AKEY $SKEY
----

.Run the mc command to list the available S3 buckets
[source, shell]
----
mc ls demouser
----

.Create a 10 MB file and upload it to the bucket
[source, shell]
----
dd if=/dev/random of=10MB.dat bs=1M count=10
mc cp 10MB.bin demouser/demo-bucket
----

.Get a bucket listing and download the 10 MB object
[source, shell]
----
mc ls demouser/demo-bucket
mc cp demouser/demo-bucket/10MB.bin GET-10MB.bin
----

.Verify the data integrity of the uploaded and downloaded files by comparing their checksums
[source, shell]
----
md5sum 10MB.bin
md5sum GET-10MB.bin
----

#### Using the RCLONE client

.Configure the RCLONE Client with the access key, secret key, and endpoint values
[source, shell]
----
export AKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].access_key');echo $AKEY
export SKEY=$(ssh ceph-node1 sudo radosgw-admin user info --uid=demouser | jq -r '.keys[0].secret_key');echo $SKEY
rclone config create demouser s3 endpoint=ceph-node4 access_key_id=$AKEY secret_access_key=$SKEY
----

.Run the RCLONE command to list the available S3 buckets
[source, shell]
----
rclone lsd demouser:
----

.Create a 10 MB file and upload it to the bucket
[source, shell]
----
dd if=/dev/random of=10MB.dat bs=1M count=10
rclone copy 10MB.bin demouser:demo-bucket --no-check-dest -v
----

.Get a bucket listing and download the 10 MB object
[source, shell]
----
rclone ls demouser:demo-bucket
rclone copyto demouser:demo-bucket/10MB.bin GET-10MB.bin -v
----

.Verify the data integrity of the uploaded and downloaded files by comparing their checksums
[source, shell]
----
md5sum 10MB.bin
md5sum GET-10MB.bin
----
